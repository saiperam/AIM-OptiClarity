{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type:ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.applications import ResNet50  \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "IMG_SIZE =224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = ['images/'+ name for name in df['name'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = to_categorical(y,num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "  img = cv2.imread(image_path)      # read image\n",
    "  if img is None:\n",
    "      print(f\"Warning: Unable to load image at {image_path}\")\n",
    "      return None\n",
    "  img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)       # convert to RGB\n",
    "  img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))    # resize for the model\n",
    "  img = img/255.0      # normalize pixel values to between 0 and 1\n",
    "\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = np.array([load_image(img) for img in image_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_images, y_labels, test_size=0.2, random_state=42) # training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomFlip('horizontal'),\n",
    "  layers.RandomRotation(0.1),\n",
    "  layers.RandomZoom(0.1)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augmented = data_augmentation(X_train, training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))  # loading model without top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False   # Freezing layers to exclude from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(5, activation='softmax', name='type')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = base_model.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=0.0001), loss=\n",
    "        'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 766ms/step - accuracy: 0.7647 - loss: 0.8236 - val_accuracy: 0.7746 - val_loss: 0.8451\n",
      "Epoch 2/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 787ms/step - accuracy: 0.7560 - loss: 0.8529 - val_accuracy: 0.7746 - val_loss: 0.8530\n",
      "Epoch 3/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 806ms/step - accuracy: 0.7548 - loss: 0.8422 - val_accuracy: 0.7746 - val_loss: 0.8553\n",
      "Epoch 4/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 823ms/step - accuracy: 0.7595 - loss: 0.8354 - val_accuracy: 0.7746 - val_loss: 0.8527\n",
      "Epoch 5/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 837ms/step - accuracy: 0.7900 - loss: 0.7493 - val_accuracy: 0.7746 - val_loss: 0.8551\n",
      "Epoch 6/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 870ms/step - accuracy: 0.7821 - loss: 0.7978 - val_accuracy: 0.7746 - val_loss: 0.8484\n",
      "Epoch 7/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 882ms/step - accuracy: 0.7860 - loss: 0.7633 - val_accuracy: 0.7746 - val_loss: 0.8555\n",
      "Epoch 8/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 890ms/step - accuracy: 0.7547 - loss: 0.8637 - val_accuracy: 0.7746 - val_loss: 0.8462\n",
      "Epoch 9/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 892ms/step - accuracy: 0.7841 - loss: 0.7802 - val_accuracy: 0.7746 - val_loss: 0.8650\n",
      "Epoch 10/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 905ms/step - accuracy: 0.7695 - loss: 0.7985 - val_accuracy: 0.7746 - val_loss: 0.8412\n",
      "Epoch 11/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 897ms/step - accuracy: 0.7765 - loss: 0.7943 - val_accuracy: 0.7746 - val_loss: 0.8514\n",
      "Epoch 12/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 908ms/step - accuracy: 0.7800 - loss: 0.7840 - val_accuracy: 0.7746 - val_loss: 0.8584\n",
      "Epoch 13/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 914ms/step - accuracy: 0.7735 - loss: 0.8020 - val_accuracy: 0.7746 - val_loss: 0.8491\n",
      "Epoch 14/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 947ms/step - accuracy: 0.7549 - loss: 0.8461 - val_accuracy: 0.7746 - val_loss: 0.8511\n",
      "Epoch 15/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 954ms/step - accuracy: 0.7672 - loss: 0.8138 - val_accuracy: 0.7746 - val_loss: 0.8484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x358934ee0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_augmented, y_train, batch_size=32, epochs=15, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 451ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7361111111111112"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "y_pred_classes = y_pred.argmax(axis=1) if y_pred.shape[1] > 1 else y_pred\n",
    "accuracy_score(y_test_classes, y_pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
